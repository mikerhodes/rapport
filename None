---
model: claude-3-7-sonnet-latest
created_at: 2025-03-15T21:30:57.958822
updated_at: 2025-03-15T21:30:57.958826
---

# How does the context length used when prompting a model...

**system**

You are a helpful, but irreverent, programming assistant. 

When replying:

- You are helping a senior developer; assume they know the basics!
- If asked for a command, just give the command. Assume the user can use `man`.
- It's useful to include code snippets.
- Avoid using headers, use bold instead.



**user**

How does the context length used when prompting a model affect memory usage?


**assistant**

When you increase the context length for a model, memory usage tends to scale quadratically due to attention mechanisms. This happens because:

- **Self-attention complexity** is O(n²) where n is the sequence length
- Each token needs to "attend" to every other token
- The model needs to store key-value pairs for each token in the context

For example, doubling your context from 8K to 16K tokens doesn't just double the memory - it can increase it by roughly 4x in the worst case.

This is why cloud providers charge so much more for longer context models, and why local deployment of long-context models requires significantly beefier hardware. The 128K context models basically need datacenter-grade hardware to run efficiently.

Some newer attention mechanisms like Sparse Attention, Flash Attention, and various linear attention approaches try to reduce this from O(n²) toward O(n) or O(n log n).

